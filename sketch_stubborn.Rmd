---
title: "First sketch of the stubborn model"
author: "Nicolas Restrepo"
date: "7/6/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)
theme_set(theme_classic())
```

## Introduction 

Here, I am going to go through a first sketch of what the "stubborn model" might look like. I am going to go through my modeling decisions in detail, mainly so that mistakes or inconsistencies can become clear. 

## The setting 

Agents are placed in a context where there are $M$ number of traits. For the sake of this brief sketch, I will use 4 traits. Each trait has an initial continuous value between 0 and 1: 

```{r}
num_traits <- 4
num_rounds <- 2
num_turns <- 100

# Create a matrix first - cols number of traits 
traits_df <- matrix(NA, 
                    nrow = num_rounds * num_turns, 
                    ncol = 2 + num_traits)
# Change the column names
colnames(traits_df) <- c("run", 
                         "generation", 
                         map_chr(.x = 1:num_traits, ~ paste0("trait", .x)))

# Turn into a tibble for ease of manipulation
traits_df <- as_tibble(traits_df)

# Populate the dataframe
traits_df <- traits_df %>% 
  mutate(run = as.factor(rep(1:num_rounds, each = num_turns)),
         generation = rep(1:num_turns, num_rounds)) %>% 
  mutate(across(.cols = 3:ncol(.), as.numeric))

head(traits_df)

```

Each trait has also a probability of changing. These probabilities are drawn from a distribution with a long right tail. This means that only few traits are likely to have a high probability of changing. To draw these probabilities, I use a beta distribution with parameters $\alpha = 1; \beta = 5$. 

```{r}
beta_df <- tibble(
x =seq(0, 1, by = 0.01),
y =dbeta(x, 0.5, 5)
)
beta_df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_line() + 
  labs(title = "Beta distribution", 
       subtitle = "alpha = 0.5; beta = 5")

```

## The agents 

We have $N$ agents. Each starts with a probabilistically appropriate response to each of the $M$ traits. To do this, I take the values for each trait and add some random noise. The agents' responses might look like this: 

```{r}
N <- 4
# Dataframe for population
  population <- matrix(NA, 
                       nrow = N, 
                       ncol = 3 + num_traits)
  # Change the column names
  colnames(population) <- c("prob_soc_learns", 
                            "prob_ind_learns",
                            "fitness", 
                            map_chr(.x = 1:num_traits, ~ paste0("trait", .x)))
  population <- as_tibble(population)
  
population
```

The agents also have different probabilities of learning. They have a probability of learning socially; that is of copying another agent's response to a given trait. They also have a probability of exploring individually: of directly accessing the true value of the trait. Both probabilities will be drawn from Beta distributions. In this example, I use the following:

```{r}
beta_df <- tibble(
x =seq(0, 1, by = 0.01),
y =dbeta(x, 2, 5)
)
beta_df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_line() + 
  labs(title = "Probability of learning socially", 
       subtitle = "alpha = 2; beta = 5")

beta_df <- tibble(
x =seq(0, 1, by = 0.01),
y =dbeta(x, 0.8, 5)
)
beta_df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_line() + 
  labs(title = "Probability of learning individually", 
       subtitle = "alpha = 0.8; beta = 5")

```


## Each turn 

Now, I am going to go through what will happen on each turn. 

### Change traits 

The first step is to determine whether traits will change. They are updated if the outcome of a Bernoulli trial with probability equal to the probabilities drawn above is successful. If they change, their values are redrawn from a Uniform distribution [0,1].

```{r, eval=FALSE}
# 1. Change in traits? 
    # Do the traits change? 
    traits_change <- rbinom(n = 4, 
                            size = 1, 
                            prob = pr_change)
    
    if(t == 1) { 
      traits_df[traits_df$run==j&traits_df$generation==t,c(which(traits_change==1)+2)] <- as.list(runif(sum(traits_change)))
    } else { 
      traits_df[traits_df$run==j&traits_df$generation==t,3:ncol(traits_df)] <- traits_df[traits_df$run==j&traits_df$generation==t-1,3:ncol(traits_df)]
      traits_df[traits_df$run==j&traits_df$generation==t,c(which(traits_change==1)+2)] <- as.list(runif(sum(traits_change)))
    }
```

### Learning 

Then we draw a trait, which will be the one that agents will be responding to on this particular turn. Then the agents learn with probability equal to their own propensity to learn. If they learn socially, they copy another random agent's response to the chosen trait. If they explore individually, they access the environment directly. The parameter `sd_exploration` controls how accurately agents learn from the environemt. This is what that will look like: 

```{r, eval=FALSE}
# 2. Draw trait & learning 
    # One trait gets drawn 
    current_trait <- sample(1:4, 
                            size = 1)
    # How many social learners?
    learns <- rbinom(n = N, 
                     size = 1, 
                     prob = population$prob_soc_learns)
    # Update these 
    population[which(learns == 1),current_trait+3] <- sample(population[,current_trait+3][[1]],
                                                             sum(learns))
    
    # Now see who explores individually 
    explorers <- rbinom(n = N, 
                        size = 1, 
                        prob = population$prob_ind_learns)
    
    # Explorers get knowledge of the trait with some error
    population[which(explorers == 1),current_trait+3] <- rnorm(n = sum(explorers), 
                                                               mean = traits_df[traits_df$run==j&traits_df$generation==t,current_trait+2][[1]], 
                                                               sd = sd_exploration)
```

### Calculate fitness 

Now, we want to calculate the fitness of the agents. The fitness should be inversely proportional to the distance between the actual value of the trait and the agent's response to it. So, the further away an agent's response is to the value of the trait, the lower its fitness. 

We can implement this as follows: 

$$ fitness = 1 - \frac{|traitvalues - agentresponses|}{M} $$
where M = the number of traits. 

```{r, eval=FALSE}
# 3. Calculate fitness 
    
    # Get difference between all traits and responses  
    # First get a matrix of the current values of the traits 
    # Same number of rows as agents 
    traits_mat <- matrix(unlist(rep(traits_df[traits_df$run==j&traits_df$generation==t,
                                              3:(2+num_traits)][,1:num_traits], N)), 
                         nrow = N, 
                         ncol = num_traits, 
                         byrow = T)
    
    # Now the fitness will be:
    # 1- the sum of absolute distances / M
    population$fitness <- 1-(rowSums(abs(population[4:ncol(population)] - traits_mat))/num_traits)

    
    # if fitnesses below 0 then 0
    
    population[population$fitness<=0,'fitness'] <- 0
```

### Reproduction 

Based on the fitness we just calculated, we make individuals reproduce. For this first sketch, I will keep the population stable. 

Agents will reproduce every certain number of turns and we can tune this in our parameter exploration. They will reproduce with probability equal to their fitness. The offspring will inherit traits and learning propensity from their parents. When agents die they are replaced by new individuals with re-initialized traits and learning probabilities. 

```{r, eval = FALSE}
# 5. Reproduction
    previous_population <- population
    population$fitness <- as.numeric(NA)
    
    if(t %% 1 == 0) {
    # Which ones survive?
    survivors <- rbinom(n = N, 
                        size = 1, 
                        prob = previous_population$fitness)
    # They reproduce 
    population[which(survivors==1),] <- previous_population[which(survivors==1),]
    
    # The rest get values re-initialized 
    population[which(survivors==0),"prob_soc_learns"] <- rbeta(sum(survivors==0), 2, 5)
    population[which(survivors==0), "prob_ind_learns"] <- rbeta(sum(survivors==0), 0.8, 5)
    for (k in 1:num_traits) {
      population[which(survivors==0),3+k] <- rnorm(n = sum(survivors==0),
                                                   mean = output[output$run==1 & 
                                                                   output$generation==1,5+k][[1]],
                                                   sd = starting_error_sd)
    }
    }
```

Now, we are at a point where we can put all this together and run this sketch. 

## The Model 

I take the model and build it as a function. 

```{r}
stubborn_model <- function(num_traits = 4,
                           num_turns = 200,
                           num_rounds = 2,
                           N = 100,
                           sd_exploration = 0.01,
                           starting_error_sd = 0.01) { 
  #### Set-Up ####

# Keep record 
# Create a matrix first - cols number of traits 
traits_df <- matrix(NA, 
                    nrow = num_rounds * num_turns, 
                    ncol = 2 + num_traits)
# Change the column names
colnames(traits_df) <- c("run", 
                         "generation", 
                         map_chr(.x = 1:num_traits, ~ paste0("trait", .x)))

# Turn into a tibble for ease of manipulation
traits_df <- as_tibble(traits_df)

# Populate the dataframe
traits_df <- traits_df %>% 
  mutate(run = as.factor(rep(1:num_rounds, each = num_turns)),
         generation = rep(1:num_turns, num_rounds)) %>% 
  mutate(across(.cols = 3:ncol(.), as.numeric))


# Output for the model
output <- matrix(NA, 
                 nrow = num_rounds * num_turns, 
                 ncol = 5 + num_traits)

# Name the columns 
colnames(output) <- c("run", 
                      "generation", 
                      "avg_soc_learning_prob",
                      "avg_ind_learning_prob", 
                      "avg_fitness",
                      map_chr(.x = 1:num_traits, ~ paste0("mean_trait", .x)))
output <- as_tibble(output)
# Populate the dataframe
output <- output %>% 
  mutate(run = as.factor(rep(1:num_rounds, each = num_turns)),
         generation = rep(1:num_turns, num_rounds)) %>% 
  mutate(across(.cols = 3:ncol(.), as.numeric))


 #### The Model ####
for (j in 1:num_rounds) {
  # Probabilities that each trait changes
  pr_change <- rbeta(n = num_traits, 
                     0.1, 
                     5)
  
  # Draw initial values for the traits 
  traits_init <- runif(n = num_traits,
                       min = 0,
                       max = 1)
  traits_df[traits_df$run==j&traits_df$generation==1,3:ncol(traits_df)] <- as.list(traits_init)
  # Dataframe for population
  population <- matrix(NA, 
                       nrow = N, 
                       ncol = 3 + num_traits)
  # Change the column names
  colnames(population) <- c("prob_soc_learns", 
                            "prob_ind_learns",
                            "fitness", 
                            map_chr(.x = 1:num_traits, ~ paste0("trait", .x)))
  population <- as_tibble(population)
  
  population <- population %>% 
    mutate(prob_soc_learns = rbeta(N, 2, 5),
           prob_ind_learns = rbeta(N, 0.8, 5),
           across(.cols = 2:ncol(.), as.numeric))
  
  # List of starting values (with slight error)
  list_starting_values <- map(.x = 1:num_traits,
                              ~ rnorm(mean = 0,
                                      sd = starting_error_sd,
                                      n = N) + rep(traits_init[.x], N))
  # Assign starting values 
  for (s in 1:num_traits) {
    population[,s+3] <- list_starting_values[[s]]
  }

  for (t in 1:num_turns) {
    #### One turn ####
    
    # 1. Change in traits? 
    # Do the traits change? 
    traits_change <- rbinom(n = 4, 
                            size = 1, 
                            prob = pr_change)
    
    if(t == 1) { 
      traits_df[traits_df$run==j&traits_df$generation==t,c(which(traits_change==1)+2)] <- as.list(runif(sum(traits_change)))
    } else { 
      traits_df[traits_df$run==j&traits_df$generation==t,3:ncol(traits_df)] <- traits_df[traits_df$run==j&traits_df$generation==t-1,3:ncol(traits_df)]
      traits_df[traits_df$run==j&traits_df$generation==t,c(which(traits_change==1)+2)] <- as.list(runif(sum(traits_change)))
    }
    
    # 2. Draw trait & learning 
    # One trait gets drawn 
    current_trait <- sample(1:4, 
                            size = 1)
    # How many social learners?
    learns <- rbinom(n = N, 
                     size = 1, 
                     prob = population$prob_soc_learns)
    # Update these 
    population[which(learns == 1),current_trait+3] <- sample(population[,current_trait+3][[1]],
                                                             sum(learns))
    
    # Now see who explores individually 
    explorers <- rbinom(n = N, 
                        size = 1, 
                        prob = population$prob_ind_learns)
    
    # Explorers get knowledge of the trait with some error
    population[which(explorers == 1),current_trait+3] <- rnorm(n = sum(explorers), 
                                                               mean = traits_df[traits_df$run==j&traits_df$generation==t,current_trait+2][[1]], 
                                                               sd = sd_exploration)
    # Record the means 
    for (m in 1:num_traits) {
      output[output$generation==t & 
               output$run==j,m+5] <- mean(population[[m+3]])
    }
    
    # 3. Calculate fitness 
    
    # Get difference between all traits and responses  
    # First get a matrix of the current values of the traits 
    # Same number of rows as agents 
    traits_mat <- matrix(unlist(rep(traits_df[traits_df$run==j&traits_df$generation==t,
                                              3:(2+num_traits)][,1:num_traits], N)), 
                         nrow = N, 
                         ncol = num_traits, 
                         byrow = T)
    
    # Now the fitness will be:
    # 1- the sum of absolute distances / M
    population$fitness <- 1-(rowSums(abs(population[4:ncol(population)] - traits_mat))/num_traits)

    
    # if fitnesses below 0 then 0
    
    population[population$fitness<=0,'fitness'] <- 0
    
    # If all fitnesses are 0, everyone dies
    if (sum(population$fitness)<= 0) {
      print("Everyone died!")
      break
    }
    
    # 4. Store population characteristics 
    output[output$generation==t & 
             output$run==j,]$avg_fitness <- mean(population$fitness, na.rm = T)
    output[output$generation==t & 
             output$run==j,]$avg_soc_learning_prob <- mean(population$prob_soc_learns)
    output[output$generation==t & 
             output$run==j,]$avg_ind_learning_prob <- mean(population$prob_ind_learns)
    
    
    # 5. Reproduction
    previous_population <- population
    population$fitness <- as.numeric(NA)
    
    if(t %% 1 == 0) {
    # Which ones survive?
    survivors <- rbinom(n = N, 
                        size = 1, 
                        prob = previous_population$fitness)
    # They reproduce 
    population[which(survivors==1),] <- previous_population[which(survivors==1),]
    
    # The rest get values re-initialized 
    population[which(survivors==0),"prob_soc_learns"] <- rbeta(sum(survivors==0), 2, 5)
    population[which(survivors==0), "prob_ind_learns"] <- rbeta(sum(survivors==0), 0.8, 5)
    for (k in 1:num_traits) {
      population[which(survivors==0),3+k] <- rnorm(n = sum(survivors==0),
                                                   mean = output[output$run==1 & 
                                                                   output$generation==1,5+k][[1]],
                                                   sd = starting_error_sd)
    }
    }
  }
  
}
  
  return(list(output, 
              traits_df, 
              pr_change))
  
}
```

Now, I am going to run it to a few runs, and visualize some of the variables that might be of interest. 

```{r}
set.seed(76)
results <- stubborn_model(num_rounds = 4)
```

I'll begin by looking at the average fitness of the population across the generations, for each run. 

```{r, echo = FALSE}
pop_df <- results[[1]]

pop_df %>% 
  ggplot(aes(x = generation, 
             y = avg_fitness)) +
  geom_line() + 
  facet_wrap(~run) +
  labs(x = 'Generation', 
       y = "Mean Fitness", 
       title = "Fitness across generations")

```

There are a couple of things to notice here. We notice that in general these lines are quite stable; all the lines hover between 0.6  and 0.9. This makes sense given that the traits probability of changing is not that high. Moreover, when traits change, they are re-initialized, so there is a chance that they will go to a value that is close to the average agent's response. 

There are two runs that are noticeably stable, 2 and 4. However, the former stabilizes around a relatively low average fitness. We might suspect that this is a case where traits changed a few times, away from the modal agent's response. Because the agents' exploration is limited, they were never quite able to lock into an ideal response again. 

The other two runs display considerably more variance. I expect that that the values of traits here varied with more frequency. 

I am going to now look at how the traits themselves changed across the runs. 

```{r, echo = FALSE}
traits_df <- results[[2]]
traits_long <- traits_df %>% 
  pivot_longer(3:6, 
               names_to = "trait", 
               values_to = "value")
pop_long <- pop_df %>% 
  pivot_longer(6:9, 
               names_to = "trait", 
               values_to = "pop_value") %>% 
  select(pop_value)

full_df <- cbind(traits_long, 
                 pop_long)

full_df %>% 
  ggplot(aes(x = generation, 
             y = value, 
             col = trait)) + 
  geom_line(alpha = 0.5)  +
  facet_wrap(~run) + 
  labs(y = "Value of trait", 
       title = "Traits across Generations")
  

```

As we suspected, runs 1 and 3 display a lot more change than the remaining two. In the first run, all traits seem to have a relatively high probability of changing. In the third run, one of the traits moves around quite a lot. In the other two runs, we only see one change across all turns. 

Let's see how the agents' responses developed across the simulations. 

```{r}
full_df %>% 
  ggplot(aes(x = generation, 
             y = pop_value, 
             col = trait)) + 
  geom_line(alpha = 0.5)  +
  facet_wrap(~run) + 
  labs(y = "Agents' Responses", 
       title = "Responses across Generations")

```

Notice the changes of trait 3 in the third run and of trait 4 in the fourth one. They roughly correspond to the changes we noticed above. But the response is fairly modest; this reflects perhaps the low probability of learning and exploration that agents have. 

## Future directions

In general there are a couple of things that I would like to fix: 

- The agents' reproductive dynamics are a bit clunky still. 

- Maybe agents' probabilities of learning (socially and individually) should also evolve?